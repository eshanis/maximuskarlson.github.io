---
title: "Logistic Regression from Scratch using Python"
date: 2020-02-12
tags: [machine-learning]
header:
    image: "/images/lake-house.jpg"
excerpt: "Create a logistic regression classifier from scratch and compare it to the sklearn version."
mathjax: "true"
---

# Logistic Regression

In statistics, the logistic model (or logit model) is used to model the probability of a certain class or event existing such as pass/fail, win/lose, alive/dead or healthy/sick. This can be extended to model several classes of events such as determining whether an image contains a cat, dog, lion, etc. Each object being detected in the image would be assigned a probability between 0 and 1 and the sum adding to one. In the Machine Learning world, Logistic Regression is a kind of parametric classification model, despite having the word ‘regression’ in its name.
This means that logistic regression models are models that have a certain fixed number of parameters that depend on the number of input features, and they output categorical prediction, like for example if a plant belongs to a certain species or not. To get a better understanding how logistic regression works I can suggest the awesome videos by *Josh Starmer* [Youtube Link to his channel](https://www.youtube.com/user/joshstarmer). As we want to focus on creating a classifier from scratch I will assume the following things:

* You have basic knowledge in python
* You have a basic understanding of how logistic regression works
* You know what backpropagation is

If that is the case, we can finally start:

## Create the dataset

Here we will use a simple dataset which can be created using the `sklearn.dataset.make_blobs` function.

![Logistic Regression dataset](/images/log-regression/data.png)

The picture above shows the two classes of the data which we will try to predict. This dataset can be reproduced by executing the following code:

```python
import numpy as np
import seaborn as sns
import sklearn.datasets as sd

# =============================================================================
# Create a dataset
# =============================================================================

data, labels = sd.make_blobs(5000, 2, 2, cluster_std=2.2, random_state=7) 

# Plot the data
sns.scatterplot(data[:, 0], data[:, 1], hue=labels)
```

After we created our dataset it is time to start writing code for our logistic classifier.

## Logistic classifier

One of the things that really often gets connected to logistic regression is the so called *Sigmoid function*. Basically this function takes an input from -$$\infty$$ to +$$\infty$$ and computes a value between $$0$$ and $$1$$. Important to understand is, that *Sigmoid function* is a wrapper for differetn functions which all produce similar output. In our case we used the *logistic function*. Below you see the function and the value range.

![logistic function](/images/log-regression/sigmoid.png)

The logistic function is defined as $$ h_ \theta (x) =  \frac{\mathrm{1} }{\mathrm{1} + e^- \theta^Tx }  $$, therefore we can simply translate that into python:

```python
def sigmoid(scores):
    return(1 / (1 + np.exp(-scores)))
```

Having this, we can focus on building the main classifier function. This function will do all neccessary calculations at once, to maintain control over what is happening let us first write down what is important, what are the inputs and what is the desired output:

* **Important:** The function must be kept *variable* in a way, that the user can modify for example the iterations and the learning rate of the backpropagation. Furthermore we want thee *weights* as an output - the parameters of the logistic function. This will enable us to compare our function for examplee with the implementation of sklearn.
* **Inputs:** To fulfill the above defined demands, our function needs to take several inputs:
    * `features` - input data
    * `target` - the corrsponding class of the input data
    * `num_steps` - how many *steps* shall we go on the backpropagation path (mathematically incorrect but easy to imagine)
    * `learning_rate` - how big are our steps which we take
* **output:** We want the weights of our logistic function to be our output.

The definition of our function will therefore look like this:

```python
def logistic_regression(features, target, num_steps, learning_rate):
    pass:
```

Having the framework of our function we now need to set up our data correctly and initialize the weights. Set-up correctly in a way that we have a y-intercept completing our two-dimensional dataset. As the feature-data is alingned horizontally we need to initialize the weights vertically with three values or in python language with `np.zeros(features.shape[1])`.
If you don't feel comfortable with the used `numpy` functions I suggest taking a look at the very detailed [documentation](https://numpy.org/doc/1.18/reference/generated/numpy.vstack.html). 

```python
intercept = np.ones((features.shape[0], 1))
features = np.hstack((intercept, features))
weights = np.zeros(features.shape[1])
```

Last but not least we need to actually iterate through all defined steps and adapt the weights to the resulting error score. This is the *machine learning* part of the logistic regression and widly known as *backpropagation*. Please check the detailed comments within the used code:

```python
for step in range(num_steps):
        
        # Calculate the scores and predict a certain class
        scores = np.dot(features, weights)
        predictions = sigmoid(scores)

        # clalculate the error
        error = target - predictions
        
        # backpropagate
        gradient = np.dot(features.T, error)
        
        # update the weights
        weights += learning_rate * gradient
        
        # give an update of the current state   
        if step % step_10 == 0:
            print('{}% done.'.format((step/num_steps) * 100))

    #return the calculated weights    
    return(weights)
```

After defining all this, it is now time to use the function and to create the first set of weights. The choosen parameters are basically random and of course can be changed. Feel free to try different sets of parameters and check the results.

```python
weights_scratch = logistic_regression(data, \
                              labels, \
                              num_steps = 100000, \
                              learning_rate = 5e-5)
```

## Compare the results with sklearn

As this is no tutorial on how to use `sklearn` we will just discuss the results here.

```bash
##########

Own Log-Reg Function parameters:
[ 8.6571122   1.5139674  -0.26879301]

##########

Sklearn results:
[8.65713384] [[ 1.51396879 -0.26879545]]

##########
```

As you can see the results are nearly exactly the same - the slight differences might  result from rounding errors. Therefore: Congratulations, you managed to create a logistic regressor which produces (the same) out put as the smart persons at `sklearn`. NOTE: It may perform similar on such a small dataset, if you try out the performance on a large dataset you will notice why everyone uses `sklearn` and not the here suggestes approach :D.

## Accuracy

And how about the *most iconic* measure when it comes to classification? How many points were classified correctly? What is our accurary percentage? To evaluate this, we will just use our set of weights and the intersect to predict labels for the initial `data` which than can easily be evaluated.

```python
data_test = np.hstack((np.ones((data.shape[0], 1)),
                                 data))
final_scores = np.dot(data_test, weights_scratch)
preds = np.round(sigmoid(final_scores))
```

The resulting accurarcy of our *from_scratch_approach* is, again, as good as the one from the `sklearn`. Makes total sense after our parameters were basically the same.

```bash
Accuracy from scratch: 0.9574
Accuracy from sk-learn: 0.9574
```

Hope you enjoyed this tutorial! The complete code can be found on my [github page](https://github.com/maximusKarlson) or directly within [this](https://github.com/maximusKarlson/python/tree/master/logistic_regression_scratch) repository.